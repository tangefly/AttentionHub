## 1 Install

```
git clone https://github.com/tangefly/AttentionHub.git
cd AttentionHub
pip install -e .
```

## 2 Support Kernels

|            kernel            |                           docs                           |
| :--------------------------: | :----------------------------------------------------------: |
| scaled_dot_product_attention | [scaled_dot_product_attention.md](https://github.com/tangefly/AttentionHub/blob/main/docs/scaled_dot_product_attention.md) |
| multi_query_attention | [multi_query_attention.md](https://github.com/tangefly/AttentionHub/blob/main/docs/multi_query_attention.md) |
| grouped_query_attention | [grouped_query_attention.md](https://github.com/tangefly/AttentionHub/blob/main/docs/grouped_query_attention.md) |
| factorized_attention | [factorized_attention.md](https://github.com/tangefly/AttentionHub/blob/main/docs/factorized_attention.md) |
| predict_sparse_attention | [predict_sparse_attention.md](https://github.com/tangefly/AttentionHub/blob/main/docs/predict_sparse_attention.md) |