## 1 基本理论

![image1](https://github.com/user-attachments/assets/73258503-4952-462f-b442-c0f4deaf7231)

Dynamic Sparse Attention 的思想是使用一个较为便宜的计算分支去计算全注意力的稀疏模式，把标准注意力的计算变成一个更为稀疏的计算，从而对稀疏矩阵的一系列计算进行优化。

这里该重点关注的是计算稀疏掩码的分支，这里的 $P$ 是一个随机初始化的稀疏映射矩阵，$P \in \sqrt{\frac{3}{k} } \cdot \left \{ -1, 0, 1 \right \} ^{d \times k}$。$\tilde{W_{Q}} \in \mathbb{R} ^{k \times k}, \tilde{W_{K}} \in \mathbb{R} ^{k \times k}$ 的作用近似于 $W_{Q}, W_{K}$，具体计算流程如下。

$$
\tilde{Q}, \tilde{K} = XP\tilde{W_{Q}}, XP\tilde{W_{K}}
$$

这里设定 $k << d$，可把矩阵计算开销大幅下降。之后 $\tilde{Q}, \tilde{K}  $ 进行矩阵计算可以得到一个近似的注意力分数，之后使用 tok-k 或者阈值的方式得到一个 $\{0, 1\}$ 掩码矩阵用于标准注意力计算。

考虑到模型的稳定性，在训练的过程中，需要对齐两个分支的注意力，这里使用

$$
L_{MSE} = \frac{1}{B} ||S-\tilde{S} ||_{2}^{2} = \frac{1}{B} ||QK^{T} - \tilde{Q} \tilde{K}^{T} ||_{2}^{2} 
$$

计算两个分支的注意力损失，达到对齐效果。

由于计算稀疏掩码的分支需要的开销较小，可以使用更低精度例如 int4 等来计算。

## 2 优点

- 使用了可训练的方式来预测稀疏模式，相对于固定的稀疏模式更合理
- 将全注意力转换为轻量级的注意力和可加速稀疏矩阵预算

## 3 缺点

- 轻量级注意力分支通过把长向量映射成短向量再计算注意力分数，有可能会损失信息，导致稀疏掩码计算的并不准确
- 轻量级分支引入的计算也是额外的计算开销



