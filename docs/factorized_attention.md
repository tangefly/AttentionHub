## 1 数学理论

首先给出如下注意力计算函数

$$
Atten(X,S)=(a(x_{i},S_{i}))_{i \in \{1,...,n \}}
$$

- $x_{i}$ 表示第 $i$ 个 token 的输入向量
- $S_{i}$ 表示第 $i$ 个 token 需要交互的 $K,V$ 张量的下标集合
- $a(x_{i},S_{i})$ 表示第 $i$ 个 token 的注意力权重向量
- $Atten(X,S)$ 表示所有的 token 的注意力权重张量

使用包括 $S_{i}$ 在内的下标集合定义这个注意力函数，目的是突出每个 token 所要关注的 $K,V$ 张量的子集。

$$
a(x_{i}, S_{i})=softmax(\frac{(W_{q}x_{i})K^{T}_{S_{i}}}{\sqrt{d} } )V_{S_{i}} \\ 
K_{S_{i}}=(W_{k}x_{j})_{j \in S_{}i} \ \ V_{S_{i}}=(W_{v}x_{j})_{j \in S_{i}}
$$

上面几个公式是注意力的常规公式，但是有了 $S_{i}$ 变量之后，每个 query 可以有选择性地与部分 $K,V$ 计算注意力权重，而不是与它之前的所有下标对于的 $K,V$ 计算注意力分数。

对于全注意力来说，$S_{i} = \left \{ j | j<=i \right \} $。

Factorized Attention 的想法是有 $p$ 个分离的注意力头，第 $m$ 个头定义了一个下标子集 $A^{(m)}_{i} \subset \left \{ j | j<=i \right \}$ 并且使 $S_{i}=A^{(m)}_{i}$。我们期望能够高效地选择子集 $A$，使得 $|A^{(m)}_{i}|\propto \sqrt[p]{n} $ 通过这种方式来大幅降低计算复杂度。

Factorized Attention 提出了注意力链的想法，对于全注意力，第 $i$ 个 token 要关注第 $j$ 个 token 是直接进行点积计算两者的相似度。而 Factorized Attention 提出需要发生注意力的两者并不需要直接进行点积计算，可以是 $(j,a,b,c,d,...,i)$ (a,b,c 这些是下标)这种传递形式。这里 $a \in A^{1}_{a}, b\in A^{2}_{b}, c \in A^{3}_{c}$ 以此类推，也就是说通过多个注意力头关注不同的局部下标的情况下，实现简洁的 token 关注，而最终实现整体的全注意力关注。

## 2 具体方案

Factorized Attention 提出使用两个注意力头，一个注意力头关注最集的 $l$ 个 token ，另一个注意力头关注固定步长的 token。

![image2](https://github.com/user-attachments/assets/8a167c1c-2c12-4b02-8c02-1a9d54b8ed28)

如上图所示，$i$ 轴表示 query 的下标，j 表示 $K,V$ 的下标。二维坐标中有颜色的小格表示 $i$ 和 $j$ 计算注意力。从图中很明显就能看出最多只需要两步就能让每个 $i$ token 关注 前 $j$ 个 token。如上图，第 14 个 $q$ 想要和第 $5$ 个 $K,V$ 发生交互，通过的路径是 $(14, 6, 5)$ 只需要两步即可。 

## 3 优点

- 可以减少就算复杂度

## 4 缺点

- 使用了固定的稀疏模式，有可能不能很好的提取信息
- 通过不同的注意力头来间接传递关注，效果可能不好，而且这种方法也很难证实注意力确实传递了。
